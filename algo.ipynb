{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaomengjin/miniconda3/envs/cs229/lib/python3.6/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from util import load_pickle_file\n",
    "from util import save_pickle_file\n",
    "from util import report_test\n",
    "from util import upsample_pos\n",
    "from util import data_preprocessing\n",
    "from util import rand_train_test\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kmeans(x, y, test=None):\n",
    "    kmeans = KMeans(n_clusters=2, random_state=229).fit(x)\n",
    "    \n",
    "    if test is not None:\n",
    "        x_test, y_test = test\n",
    "        # clf_acc = report_test(kmeans, test, \"kmeans\")\n",
    "        # print(kmeans.cluster_centers_)\n",
    "        y_pred = kmeans.predict(x_test)\n",
    "        print((y_pred == y_test).sum()/len(y_test))\n",
    "        return kmeans.labels_, y_pred\n",
    "    return kmeans, test, 'kmeans'\n",
    "\n",
    "def train_svm(x, y, kernel_type, test=None):\n",
    "    clf_svm = SVC(kernel='linear', probability=True)\n",
    "    if kernel_type == 'poly':\n",
    "        clf_svm = SVC(kernel='poly', degree=8, probability=True)\n",
    "    elif kernel_type == 'rbf':\n",
    "        clf_svm = SVC(kernel='rbf', probability=True)\n",
    "    elif kernel_type == 'sigmoid':\n",
    "        clf_svm = SVC(kernel='sigmoid', probability=True)\n",
    "    clf_svm.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_svm, test, \"svm\")\n",
    "        return clf_svm, test, 'svm'\n",
    "    return clf_svm, test, 'svm'\n",
    "\n",
    "def train_lr(x, y, rand_state=229, solver='liblinear',\n",
    "        max_iter=10000, test=None):\n",
    "    clf_lr = LogisticRegression(\n",
    "        random_state=rand_state, solver=solver, max_iter=max_iter, C=0.0001)\n",
    "    # clf_lr = LogisticRegression(C = 0.0001)\n",
    "    clf_lr.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lr, test, \"logistic regression\")\n",
    "        return clf_lr, clf_acc\n",
    "    return clf_lr\n",
    "\n",
    "def train_rand_forest(x, y, n_est=100, max_depth=3, rand_state=229, test=None):\n",
    "    # clf_rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_depth,\n",
    "    #     random_state=rand_state)\n",
    "    clf_rf = RandomForestClassifier(n_estimators = 100, random_state = 50, n_jobs = -1)\n",
    "    clf_rf.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_rf, test, \"random forest\")\n",
    "        return clf_rf, clf_acc\n",
    "    return clf_rf\n",
    "\n",
    "def train_nb(x, y, test=None):\n",
    "    clf_nb = GaussianNB().fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nb, test, \"Gaussian Naive Bayes\")\n",
    "        return clf_nb, clf_acc\n",
    "    return clf_nb\n",
    "\n",
    "def train_mlp(x, y, solver='lbfgs', alpha=1e-4, hls=(10, 40, 40),\n",
    "        rand_state=229, test=None):\n",
    "    clf_nn = MLPClassifier(\n",
    "        solver=solver, alpha=alpha, hidden_layer_sizes=hls,\n",
    "        random_state=rand_state)\n",
    "    clf_nn.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nn, test, \"neural network\")\n",
    "        return clf_nn, clf_acc\n",
    "    return clf_nn\n",
    "\n",
    "def train_lgbm(x, y, test=None):\n",
    "    clf_lgbm = LGBMClassifier(\n",
    "        nthread=4,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=34,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.041545473,\n",
    "        reg_lambda=0.0735294,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=39.3259775,\n",
    "        silent=-1,\n",
    "        verbose=-1, )\n",
    "    clf_lgbm.fit(x, y, verbose=100)\n",
    "\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lgbm, test, \"LGBM\")\n",
    "        return clf_lgbm, clf_acc\n",
    "    return clf_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has been successfully loaded\n"
     ]
    }
   ],
   "source": [
    "training_data_path = './data_processed/training_data.pkl'\n",
    "label_path = './data_processed/training_lbl.pkl'\n",
    "# training_data_path = './data_processed/training_data_processed.pkl'\n",
    "# label_path = './data_processed/training_lbl_processed.pkl'\n",
    "# training_data_path = './data_processed/training_data.pkl'\n",
    "# label_path = './data_processed/training_lbl.pkl'\n",
    "data = load_pickle_file(training_data_path)\n",
    "label = load_pickle_file(label_path)\n",
    "print('Training data has been successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 486)\n",
      "(307511, 486)\n",
      "(307511,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(label)\n",
    "x = data\n",
    "# entries = list(data.columns)\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "# raise\n",
    "x, y = data_preprocessing(x, y, thres=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is starting ... \n",
      "shape of x: (307511, 486)\n"
     ]
    }
   ],
   "source": [
    "lr_acc_ls = []\n",
    "rf_acc_ls = []\n",
    "nb_acc_ls = []\n",
    "nn_acc_ls = []\n",
    "lgbm_acc_ls = []\n",
    "# kf = KFold(n_splits=1, shuffle=True)\n",
    "print('Training is starting ... ')\n",
    "print('shape of x: {}'.format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, x_test, y_test = upsample_pos(x, y, upsample=True)\n",
    "# x, y, x_test, y_test = rand_train_test(x, y)\n",
    "# save_pickle_file(x, \"training_data_up.pkl\")\n",
    "# save_pickle_file(y, \"training_lbl_up.pkl\")\n",
    "# save_pickle_file(x_test, \"testing_data_up.pkl\")\n",
    "# save_pickle_file(y_test, \"testing_lbl_up.pkl\")\n",
    "# x = load_pickle_file('training_data_up.pkl')\n",
    "# y = load_pickle_file('training_lbl_up.pkl')\n",
    "# x_test = load_pickle_file('testing_data_up.pkl')\n",
    "# y_test = load_pickle_file('testing_lbl_up.pkl')\n",
    "# raise\n",
    "# print('Percentage of zeros in trainset input: {}'.format(np.count_nonzero(x==0)/x.size))\n",
    "# print('Number of positive examples: {}, negative: {}'.format((y==1).sum(), (y==0).sum()))\n",
    "# # for train, test in kf.split(x):\n",
    "# print(\"here\")\n",
    "# x_train, x_test, y_train, y_test = x, x_test, y, y_test\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(len(y_test==1))\n",
    "# print(len(y_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x, y, upsample=False, k_neighbors=1000):\n",
    "    # less positive, more negative\n",
    "    all_pos = np.where(y == 1)\n",
    "    print(len(all_pos))\n",
    "    x_all_pos = x[all_pos[0]]\n",
    "    y_all_pos = y[all_pos[0]]\n",
    "\n",
    "    all_neg = np.where(y == 0)\n",
    "    print(len(all_neg))\n",
    "    x_all_neg = x[all_neg[0]]\n",
    "    y_all_neg = y[all_neg[0]]\n",
    "\n",
    "    if upsample:\n",
    "        rand_ind = np.arange(len(x_all_neg))\n",
    "        np.random.shuffle(rand_ind)\n",
    "        x_neg_new = x_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        y_neg_new = y_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        x_all_new = np.concatenate((x_neg_new, x_all_pos), axis=0)\n",
    "        y_all_new = np.concatenate((y_neg_new, y_all_pos), axis=0)\n",
    "        sm = SMOTE(random_state=233333, sampling_strategy=1.0, k_neighbors=k_neighbors)\n",
    "        x_train, y_train = sm.fit_sample(x_all_new, y_all_new)\n",
    "    else:\n",
    "        # undersample: balance train set\n",
    "#         x_all_neg = x_all_neg[:int(5*len(x_all_pos))]\n",
    "#         y_all_neg = y_all_neg[:int(5*len(x_all_pos))]\n",
    "        x_all_neg = x_all_neg[:len(x_all_pos)]\n",
    "        y_all_neg = y_all_neg[:len(x_all_pos)]\n",
    "        x_train = np.concatenate((x_all_neg, x_all_pos), axis=0)\n",
    "        y_train = np.concatenate((y_all_neg, y_all_pos), axis=0)\n",
    "    \n",
    "    rand_shuffle = np.arange(len(x_train))\n",
    "    np.random.shuffle(rand_shuffle)\n",
    "    x_train = x_train[rand_shuffle]\n",
    "    y_train = y_train[rand_shuffle]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(x):\n",
    "    b = np.bincount(x)\n",
    "    ii = np.nonzero(b)[0]\n",
    "    print(np.vstack((ii,b[ii])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825\n",
      "282686\n",
      "0.5314199395770393\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "length of train set 4742, test set 1238\n",
      "train 1 4742, train 0 4742\n",
      "The accuracy for logistic regression classifier is: 0.6591276252019386\n",
      "Prediction Positive Number: 485 True Number: 619\n",
      "Prediction Negative Number: 753 True Number: 619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69       619\n",
      "           1       0.70      0.55      0.62       619\n",
      "\n",
      "    accuracy                           0.66      1238\n",
      "   macro avg       0.67      0.66      0.66      1238\n",
      "weighted avg       0.67      0.66      0.66      1238\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6462035541195477\n",
      "Prediction Positive Number: 183 True Number: 619\n",
      "Prediction Negative Number: 1055 True Number: 619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       619\n",
      "           1       0.99      0.29      0.45       619\n",
      "\n",
      "    accuracy                           0.65      1238\n",
      "   macro avg       0.79      0.65      0.60      1238\n",
      "weighted avg       0.79      0.65      0.60      1238\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "length of train set 33681, test set 8380\n",
      "train 1 33681, train 0 33681\n",
      "The accuracy for logistic regression classifier is: 0.698926014319809\n",
      "Prediction Positive Number: 4045 True Number: 4346\n",
      "Prediction Negative Number: 4335 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.70      4034\n",
      "           1       0.73      0.68      0.70      4346\n",
      "\n",
      "    accuracy                           0.70      8380\n",
      "   macro avg       0.70      0.70      0.70      8380\n",
      "weighted avg       0.70      0.70      0.70      8380\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6231503579952268\n",
      "Prediction Positive Number: 1188 True Number: 4346\n",
      "Prediction Negative Number: 7192 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72      4034\n",
      "           1       1.00      0.27      0.43      4346\n",
      "\n",
      "    accuracy                           0.62      8380\n",
      "   macro avg       0.78      0.64      0.57      8380\n",
      "weighted avg       0.79      0.62      0.57      8380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=1000)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    cur_test_x, cur_test_y = balance_data(cur_test_x, cur_test_y, upsample=False, k_neighbors=1000)\n",
    "    \n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    print('train 1 {}, train 0 {}'.format(len(cur_train_y==1), len(cur_train_y==0)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825\n",
      "282686\n",
      "0.5314199395770393\n",
      "[[   0 3666]\n",
      " [   1 2371]]\n",
      "1\n",
      "1\n",
      "[[   0 2371]\n",
      " [   1 2371]]\n",
      "length of train set 4742, test set 1550\n",
      "The accuracy for logistic regression classifier is: 0.6896774193548387\n",
      "Prediction Positive Number: 544 True Number: 619\n",
      "Prediction Negative Number: 1006 True Number: 931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75       931\n",
      "           1       0.63      0.55      0.59       619\n",
      "\n",
      "    accuracy                           0.69      1550\n",
      "   macro avg       0.68      0.67      0.67      1550\n",
      "weighted avg       0.68      0.69      0.69      1550\n",
      "\n",
      "The accuracy for random forest classifier is: 0.7141935483870968\n",
      "Prediction Positive Number: 192 True Number: 619\n",
      "Prediction Negative Number: 1358 True Number: 931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.99      0.81       931\n",
      "           1       0.96      0.30      0.45       619\n",
      "\n",
      "    accuracy                           0.71      1550\n",
      "   macro avg       0.82      0.64      0.63      1550\n",
      "weighted avg       0.79      0.71      0.67      1550\n",
      "\n",
      "[[    0 16193]\n",
      " [    1 17488]]\n",
      "1\n",
      "1\n",
      "[[    0 16193]\n",
      " [    1 17488]]\n",
      "length of train set 33681, test set 8380\n",
      "The accuracy for logistic regression classifier is: 0.698926014319809\n",
      "Prediction Positive Number: 4045 True Number: 4346\n",
      "Prediction Negative Number: 4335 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.70      4034\n",
      "           1       0.73      0.68      0.70      4346\n",
      "\n",
      "    accuracy                           0.70      8380\n",
      "   macro avg       0.70      0.70      0.70      8380\n",
      "weighted avg       0.70      0.70      0.70      8380\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6235083532219571\n",
      "Prediction Positive Number: 1191 True Number: 4346\n",
      "Prediction Negative Number: 7189 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72      4034\n",
      "           1       1.00      0.27      0.43      4346\n",
      "\n",
      "    accuracy                           0.62      8380\n",
      "   macro avg       0.78      0.64      0.57      8380\n",
      "weighted avg       0.79      0.62      0.57      8380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    count_values(cur_train_y)\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=500)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    \n",
    "    count_values(cur_train_y)\n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
