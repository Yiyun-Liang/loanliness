{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaomengjin/miniconda3/envs/cs229/lib/python3.6/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from util import load_pickle_file\n",
    "from util import save_pickle_file\n",
    "from util import report_test\n",
    "from util import upsample_pos\n",
    "from util import data_preprocessing\n",
    "from util import rand_train_test\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kmeans(x, y, n_clu=2, test=None):\n",
    "    kmeans = KMeans(n_clusters=n_clu, random_state=229).fit(x)\n",
    "    \n",
    "    if test is not None:\n",
    "        x_test, y_test = test\n",
    "        # clf_acc = report_test(kmeans, test, \"kmeans\")\n",
    "        # print(kmeans.cluster_centers_)\n",
    "        y_pred = kmeans.predict(x_test)\n",
    "        print((y_pred == y_test).sum()/len(y_test))\n",
    "        return kmeans.labels_, y_pred\n",
    "    return kmeans, test, 'kmeans'\n",
    "\n",
    "def train_svm(x, y, kernel_type, test=None):\n",
    "    clf_svm = SVC(kernel='linear', probability=True)\n",
    "    if kernel_type == 'poly':\n",
    "        clf_svm = SVC(kernel='poly', degree=8, probability=True)\n",
    "    elif kernel_type == 'rbf':\n",
    "        clf_svm = SVC(kernel='rbf', probability=True)\n",
    "    elif kernel_type == 'sigmoid':\n",
    "        clf_svm = SVC(kernel='sigmoid', probability=True)\n",
    "    clf_svm.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_svm, test, \"svm\")\n",
    "        return clf_svm, test, 'svm'\n",
    "    return clf_svm, test, 'svm'\n",
    "\n",
    "def train_lr(x, y, rand_state=229, solver='liblinear',\n",
    "        max_iter=10000, test=None):\n",
    "    clf_lr = LogisticRegression(\n",
    "        random_state=rand_state, solver=solver, max_iter=max_iter, C=0.0001, class_weight=\"balanced\")\n",
    "    # clf_lr = LogisticRegression(C = 0.0001)\n",
    "    clf_lr.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lr, test, \"logistic regression\")\n",
    "        return clf_lr, clf_acc\n",
    "    return clf_lr\n",
    "\n",
    "def train_rand_forest(x, y, n_est=100, max_depth=3, rand_state=229, test=None):\n",
    "    # clf_rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_depth,\n",
    "    #     random_state=rand_state)\n",
    "    clf_rf = RandomForestClassifier(n_estimators = 100, random_state = 50, n_jobs = -1, class_weight=\"balanced\")\n",
    "    clf_rf.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_rf, test, \"random forest\")\n",
    "        return clf_rf, clf_acc\n",
    "    return clf_rf\n",
    "\n",
    "def train_nb(x, y, test=None):\n",
    "    clf_nb = GaussianNB().fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nb, test, \"Gaussian Naive Bayes\")\n",
    "        return clf_nb, clf_acc\n",
    "    return clf_nb\n",
    "\n",
    "def train_mlp(x, y, solver='lbfgs', alpha=1e-4, hls=(10, 40, 40),\n",
    "        rand_state=229, test=None):\n",
    "    clf_nn = MLPClassifier(\n",
    "        solver=solver, alpha=alpha, hidden_layer_sizes=hls,\n",
    "        random_state=rand_state)\n",
    "    clf_nn.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nn, test, \"neural network\")\n",
    "        return clf_nn, clf_acc\n",
    "    return clf_nn\n",
    "\n",
    "def train_lgbm(x, y, test=None):\n",
    "    clf_lgbm = LGBMClassifier(\n",
    "        nthread=4,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=34,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.041545473,\n",
    "        reg_lambda=0.0735294,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=39.3259775,\n",
    "        silent=-1,\n",
    "        verbose=-1, )\n",
    "    clf_lgbm.fit(x, y, verbose=100)\n",
    "\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lgbm, test, \"LGBM\")\n",
    "        return clf_lgbm, clf_acc\n",
    "    return clf_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has been successfully loaded\n"
     ]
    }
   ],
   "source": [
    "training_data_path = './data_processed/training_data.pkl'\n",
    "label_path = './data_processed/training_lbl.pkl'\n",
    "# training_data_path = './data_processed/training_data_processed.pkl'\n",
    "# label_path = './data_processed/training_lbl_processed.pkl'\n",
    "# training_data_path = './data_processed/training_data.pkl'\n",
    "# label_path = './data_processed/training_lbl.pkl'\n",
    "data = load_pickle_file(training_data_path)\n",
    "label = load_pickle_file(label_path)\n",
    "print('Training data has been successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 651)\n",
      "(307511, 651)\n",
      "(307511,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(label)\n",
    "x = data\n",
    "# entries = list(data.columns)\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "# raise\n",
    "x, y = data_preprocessing(x, y, thres=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is starting ... \n",
      "shape of x: (307511, 651)\n"
     ]
    }
   ],
   "source": [
    "lr_acc_ls = []\n",
    "rf_acc_ls = []\n",
    "nb_acc_ls = []\n",
    "nn_acc_ls = []\n",
    "lgbm_acc_ls = []\n",
    "# kf = KFold(n_splits=1, shuffle=True)\n",
    "print('Training is starting ... ')\n",
    "print('shape of x: {}'.format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, x_test, y_test = upsample_pos(x, y, upsample=True)\n",
    "# x, y, x_test, y_test = rand_train_test(x, y)\n",
    "# save_pickle_file(x, \"training_data_up.pkl\")\n",
    "# save_pickle_file(y, \"training_lbl_up.pkl\")\n",
    "# save_pickle_file(x_test, \"testing_data_up.pkl\")\n",
    "# save_pickle_file(y_test, \"testing_lbl_up.pkl\")\n",
    "# x = load_pickle_file('training_data_up.pkl')\n",
    "# y = load_pickle_file('training_lbl_up.pkl')\n",
    "# x_test = load_pickle_file('testing_data_up.pkl')\n",
    "# y_test = load_pickle_file('testing_lbl_up.pkl')\n",
    "# raise\n",
    "# print('Percentage of zeros in trainset input: {}'.format(np.count_nonzero(x==0)/x.size))\n",
    "# print('Number of positive examples: {}, negative: {}'.format((y==1).sum(), (y==0).sum()))\n",
    "# # for train, test in kf.split(x):\n",
    "# print(\"here\")\n",
    "# x_train, x_test, y_train, y_test = x, x_test, y, y_test\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(len(y_test==1))\n",
    "# print(len(y_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x, y, upsample=False, k_neighbors=1000):\n",
    "    # less positive, more negative\n",
    "    all_pos = np.where(y == 1)\n",
    "    print(len(all_pos))\n",
    "    x_all_pos = x[all_pos[0]]\n",
    "    y_all_pos = y[all_pos[0]]\n",
    "\n",
    "    all_neg = np.where(y == 0)\n",
    "    print(len(all_neg))\n",
    "    x_all_neg = x[all_neg[0]]\n",
    "    y_all_neg = y[all_neg[0]]\n",
    "\n",
    "    if upsample:\n",
    "        rand_ind = np.arange(len(x_all_neg))\n",
    "        np.random.shuffle(rand_ind)\n",
    "        x_neg_new = x_all_neg[rand_ind[:5*len(x_all_pos)]]\n",
    "        y_neg_new = y_all_neg[rand_ind[:5*len(x_all_pos)]]\n",
    "        x_all_new = np.concatenate((x_neg_new, x_all_pos), axis=0)\n",
    "        y_all_new = np.concatenate((y_neg_new, y_all_pos), axis=0)\n",
    "        sm = SMOTE(random_state=233333, sampling_strategy=1.0, k_neighbors=k_neighbors)\n",
    "        x_train, y_train = sm.fit_sample(x_all_new, y_all_new)\n",
    "    else:\n",
    "        # undersample: balance train set\n",
    "#         x_all_neg = x_all_neg[:int(5*len(x_all_pos))]\n",
    "#         y_all_neg = y_all_neg[:int(5*len(x_all_pos))]\n",
    "        x_all_neg = x_all_neg[:5 *len(x_all_pos)]\n",
    "        y_all_neg = y_all_neg[:5 *len(x_all_pos)]\n",
    "        x_train = np.concatenate((x_all_neg, x_all_pos), axis=0)\n",
    "        y_train = np.concatenate((y_all_neg, y_all_pos), axis=0)\n",
    "    \n",
    "    rand_shuffle = np.arange(len(x_train))\n",
    "    np.random.shuffle(rand_shuffle)\n",
    "    x_train = x_train[rand_shuffle]\n",
    "    y_train = y_train[rand_shuffle]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(x):\n",
    "    b = np.bincount(x)\n",
    "    ii = np.nonzero(b)[0]\n",
    "    print(np.vstack((ii,b[ii])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825\n",
      "282686\n",
      "0.5313192346424975\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "length of train set 4744, test set 1240\n",
      "train 1 4744, train 0 4744\n",
      "The accuracy for logistic regression classifier is: 0.6645161290322581\n",
      "Prediction Positive Number: 510 True Number: 620\n",
      "Prediction Negative Number: 730 True Number: 620\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69       620\n",
      "           1       0.70      0.58      0.63       620\n",
      "\n",
      "    accuracy                           0.66      1240\n",
      "   macro avg       0.67      0.66      0.66      1240\n",
      "weighted avg       0.67      0.66      0.66      1240\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6612903225806451\n",
      "Prediction Positive Number: 218 True Number: 620\n",
      "Prediction Negative Number: 1022 True Number: 620\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.99      0.74       620\n",
      "           1       0.96      0.34      0.50       620\n",
      "\n",
      "    accuracy                           0.66      1240\n",
      "   macro avg       0.78      0.66      0.62      1240\n",
      "weighted avg       0.78      0.66      0.62      1240\n",
      "\n",
      "The accuracy for Gaussian Naive Bayes classifier is: 0.5911290322580646\n",
      "Prediction Positive Number: 981 True Number: 620\n",
      "Prediction Negative Number: 259 True Number: 620\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.30      0.42       620\n",
      "           1       0.56      0.88      0.68       620\n",
      "\n",
      "    accuracy                           0.59      1240\n",
      "   macro avg       0.64      0.59      0.55      1240\n",
      "weighted avg       0.64      0.59      0.55      1240\n",
      "\n",
      "The accuracy for neural network classifier is: 0.5943548387096774\n",
      "Prediction Positive Number: 117 True Number: 620\n",
      "Prediction Negative Number: 1123 True Number: 620\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71       620\n",
      "           1       1.00      0.19      0.32       620\n",
      "\n",
      "    accuracy                           0.59      1240\n",
      "   macro avg       0.78      0.59      0.51      1240\n",
      "weighted avg       0.78      0.59      0.51      1240\n",
      "\n",
      "The accuracy for LGBM classifier is: 0.5\n",
      "Prediction Positive Number: 0 True Number: 620\n",
      "Prediction Negative Number: 1240 True Number: 620\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       620\n",
      "           1       0.00      0.00      0.00       620\n",
      "\n",
      "    accuracy                           0.50      1240\n",
      "   macro avg       0.25      0.50      0.33      1240\n",
      "weighted avg       0.25      0.50      0.33      1240\n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaomengjin/miniconda3/envs/cs229/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "length of train set 33680, test set 8379\n",
      "train 1 33680, train 0 33680\n",
      "The accuracy for logistic regression classifier is: 0.6986513903807137\n",
      "Prediction Positive Number: 4030 True Number: 4345\n",
      "Prediction Negative Number: 4349 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70      4034\n",
      "           1       0.73      0.67      0.70      4345\n",
      "\n",
      "    accuracy                           0.70      8379\n",
      "   macro avg       0.70      0.70      0.70      8379\n",
      "weighted avg       0.70      0.70      0.70      8379\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6231053825038787\n",
      "Prediction Positive Number: 1193 True Number: 4345\n",
      "Prediction Negative Number: 7186 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72      4034\n",
      "           1       1.00      0.27      0.43      4345\n",
      "\n",
      "    accuracy                           0.62      8379\n",
      "   macro avg       0.78      0.64      0.57      8379\n",
      "weighted avg       0.79      0.62      0.57      8379\n",
      "\n",
      "The accuracy for Gaussian Naive Bayes classifier is: 0.5061463181763933\n",
      "Prediction Positive Number: 481 True Number: 4345\n",
      "Prediction Negative Number: 7898 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.97      0.65      4034\n",
      "           1       0.72      0.08      0.14      4345\n",
      "\n",
      "    accuracy                           0.51      8379\n",
      "   macro avg       0.60      0.52      0.40      8379\n",
      "weighted avg       0.61      0.51      0.39      8379\n",
      "\n",
      "The accuracy for neural network classifier is: 0.6197636949516648\n",
      "Prediction Positive Number: 1159 True Number: 4345\n",
      "Prediction Negative Number: 7220 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72      4034\n",
      "           1       1.00      0.27      0.42      4345\n",
      "\n",
      "    accuracy                           0.62      8379\n",
      "   macro avg       0.78      0.63      0.57      8379\n",
      "weighted avg       0.79      0.62      0.56      8379\n",
      "\n",
      "The accuracy for LGBM classifier is: 0.4943310657596372\n",
      "Prediction Positive Number: 108 True Number: 4345\n",
      "Prediction Negative Number: 8271 True Number: 4034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66      4034\n",
      "           1       1.00      0.02      0.05      4345\n",
      "\n",
      "    accuracy                           0.49      8379\n",
      "   macro avg       0.74      0.51      0.35      8379\n",
      "weighted avg       0.75      0.49      0.34      8379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=1000)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    cur_test_x, cur_test_y = balance_data(cur_test_x, cur_test_y, upsample=False, k_neighbors=1000)\n",
    "    \n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    print('train 1 {}, train 0 {}'.format(len(cur_train_y==1), len(cur_train_y==0)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_nb, nb_acc = train_nb(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_mlp, mlp_acc = train_mlp(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_lgbm, lgbm_acc = train_lgbm(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825\n",
      "282686\n",
      "0.26163141993957706\n",
      "[[   0 3654]\n",
      " [   1 2368]]\n",
      "1\n",
      "1\n",
      "[[   0 3654]\n",
      " [   1 2368]]\n",
      "length of train set 6022, test set 1546\n",
      "The accuracy for logistic regression classifier is: 0.6824062095730918\n",
      "Prediction Positive Number: 574 True Number: 617\n",
      "Prediction Negative Number: 972 True Number: 929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       929\n",
      "           1       0.61      0.57      0.59       617\n",
      "\n",
      "    accuracy                           0.68      1546\n",
      "   macro avg       0.67      0.66      0.66      1546\n",
      "weighted avg       0.68      0.68      0.68      1546\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6940491591203105\n",
      "Prediction Positive Number: 148 True Number: 617\n",
      "Prediction Negative Number: 1398 True Number: 929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.80       929\n",
      "           1       0.99      0.24      0.38       617\n",
      "\n",
      "    accuracy                           0.69      1546\n",
      "   macro avg       0.82      0.62      0.59      1546\n",
      "weighted avg       0.79      0.69      0.63      1546\n",
      "\n",
      "The accuracy for Gaussian Naive Bayes classifier is: 0.6785252263906857\n",
      "Prediction Positive Number: 426 True Number: 617\n",
      "Prediction Negative Number: 1120 True Number: 929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.84      0.76       929\n",
      "           1       0.64      0.44      0.52       617\n",
      "\n",
      "    accuracy                           0.68      1546\n",
      "   macro avg       0.67      0.64      0.64      1546\n",
      "weighted avg       0.67      0.68      0.66      1546\n",
      "\n",
      "The accuracy for neural network classifier is: 0.6791720569210866\n",
      "Prediction Positive Number: 177 True Number: 617\n",
      "Prediction Negative Number: 1369 True Number: 929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.97      0.78       929\n",
      "           1       0.84      0.24      0.38       617\n",
      "\n",
      "    accuracy                           0.68      1546\n",
      "   macro avg       0.75      0.61      0.58      1546\n",
      "weighted avg       0.73      0.68      0.62      1546\n",
      "\n",
      "The accuracy for LGBM classifier is: 0.8234152652005174\n",
      "Prediction Positive Number: 344 True Number: 617\n",
      "Prediction Negative Number: 1202 True Number: 929\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       929\n",
      "           1       1.00      0.56      0.72       617\n",
      "\n",
      "    accuracy                           0.82      1546\n",
      "   macro avg       0.89      0.78      0.79      1546\n",
      "weighted avg       0.86      0.82      0.81      1546\n",
      "\n",
      "[[   0 7914]\n",
      " [   1 6926]]\n",
      "1\n",
      "1\n",
      "[[   0 7914]\n",
      " [   1 6926]]\n",
      "length of train set 14840, test set 3668\n",
      "The accuracy for logistic regression classifier is: 0.680752453653217\n",
      "Prediction Positive Number: 1428 True Number: 1669\n",
      "Prediction Negative Number: 2240 True Number: 1999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      1999\n",
      "           1       0.67      0.58      0.62      1669\n",
      "\n",
      "    accuracy                           0.68      3668\n",
      "   macro avg       0.68      0.67      0.67      3668\n",
      "weighted avg       0.68      0.68      0.68      3668\n",
      "\n",
      "The accuracy for random forest classifier is: 0.660577971646674\n",
      "Prediction Positive Number: 424 True Number: 1669\n",
      "Prediction Negative Number: 3244 True Number: 1999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76      1999\n",
      "           1       1.00      0.25      0.41      1669\n",
      "\n",
      "    accuracy                           0.66      3668\n",
      "   macro avg       0.81      0.63      0.58      3668\n",
      "weighted avg       0.79      0.66      0.60      3668\n",
      "\n",
      "The accuracy for Gaussian Naive Bayes classifier is: 0.5583424209378408\n",
      "Prediction Positive Number: 203 True Number: 1669\n",
      "Prediction Negative Number: 3465 True Number: 1999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.96      0.70      1999\n",
      "           1       0.62      0.08      0.13      1669\n",
      "\n",
      "    accuracy                           0.56      3668\n",
      "   macro avg       0.59      0.52      0.42      3668\n",
      "weighted avg       0.58      0.56      0.44      3668\n",
      "\n",
      "The accuracy for neural network classifier is: 0.6393129770992366\n",
      "Prediction Positive Number: 346 True Number: 1669\n",
      "Prediction Negative Number: 3322 True Number: 1999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75      1999\n",
      "           1       1.00      0.21      0.34      1669\n",
      "\n",
      "    accuracy                           0.64      3668\n",
      "   macro avg       0.80      0.60      0.55      3668\n",
      "weighted avg       0.78      0.64      0.57      3668\n",
      "\n",
      "The accuracy for LGBM classifier is: 0.658669574700109\n",
      "Prediction Positive Number: 417 True Number: 1669\n",
      "Prediction Negative Number: 3251 True Number: 1999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76      1999\n",
      "           1       1.00      0.25      0.40      1669\n",
      "\n",
      "    accuracy                           0.66      3668\n",
      "   macro avg       0.81      0.62      0.58      3668\n",
      "weighted avg       0.79      0.66      0.60      3668\n",
      "\n",
      "[[   0 5010]\n",
      " [   1 5516]]\n",
      "1\n",
      "1\n",
      "[[   0 5010]\n",
      " [   1 5516]]\n",
      "length of train set 10526, test set 2601\n",
      "The accuracy for logistic regression classifier is: 0.7012687427912342\n",
      "Prediction Positive Number: 1160 True Number: 1365\n",
      "Prediction Negative Number: 1441 True Number: 1236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71      1236\n",
      "           1       0.75      0.64      0.69      1365\n",
      "\n",
      "    accuracy                           0.70      2601\n",
      "   macro avg       0.71      0.70      0.70      2601\n",
      "weighted avg       0.71      0.70      0.70      2601\n",
      "\n",
      "The accuracy for random forest classifier is: 0.6382160707420222\n",
      "Prediction Positive Number: 436 True Number: 1365\n",
      "Prediction Negative Number: 2165 True Number: 1236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.72      1236\n",
      "           1       0.99      0.32      0.48      1365\n",
      "\n",
      "    accuracy                           0.64      2601\n",
      "   macro avg       0.78      0.66      0.60      2601\n",
      "weighted avg       0.79      0.64      0.59      2601\n",
      "\n",
      "The accuracy for Gaussian Naive Bayes classifier is: 0.499038831218762\n",
      "Prediction Positive Number: 150 True Number: 1365\n",
      "Prediction Negative Number: 2451 True Number: 1236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.96      0.65      1236\n",
      "           1       0.71      0.08      0.14      1365\n",
      "\n",
      "    accuracy                           0.50      2601\n",
      "   macro avg       0.60      0.52      0.39      2601\n",
      "weighted avg       0.60      0.50      0.38      2601\n",
      "\n",
      "The accuracy for neural network classifier is: 0.6155324875048058\n",
      "Prediction Positive Number: 365 True Number: 1365\n",
      "Prediction Negative Number: 2236 True Number: 1236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      1.00      0.71      1236\n",
      "           1       1.00      0.27      0.42      1365\n",
      "\n",
      "    accuracy                           0.62      2601\n",
      "   macro avg       0.78      0.63      0.57      2601\n",
      "weighted avg       0.79      0.62      0.56      2601\n",
      "\n",
      "The accuracy for LGBM classifier is: 0.7112648981161092\n",
      "Prediction Positive Number: 614 True Number: 1365\n",
      "Prediction Negative Number: 1987 True Number: 1236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77      1236\n",
      "           1       1.00      0.45      0.62      1365\n",
      "\n",
      "    accuracy                           0.71      2601\n",
      "   macro avg       0.81      0.72      0.69      2601\n",
      "weighted avg       0.82      0.71      0.69      2601\n",
      "\n",
      "[[   0 3281]\n",
      " [   1 5049]]\n",
      "1\n",
      "1\n",
      "[[   0 3281]\n",
      " [   1 5049]]\n",
      "length of train set 8330, test set 2115\n",
      "The accuracy for logistic regression classifier is: 0.6685579196217494\n",
      "Prediction Positive Number: 1021 True Number: 1314\n",
      "Prediction Negative Number: 1094 True Number: 801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.75      0.63       801\n",
      "           1       0.80      0.62      0.70      1314\n",
      "\n",
      "    accuracy                           0.67      2115\n",
      "   macro avg       0.67      0.68      0.66      2115\n",
      "weighted avg       0.70      0.67      0.67      2115\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for random forest classifier is: 0.5687943262411348\n",
      "Prediction Positive Number: 436 True Number: 1314\n",
      "Prediction Negative Number: 1679 True Number: 801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.98      0.63       801\n",
      "           1       0.96      0.32      0.48      1314\n",
      "\n",
      "    accuracy                           0.57      2115\n",
      "   macro avg       0.71      0.65      0.56      2115\n",
      "weighted avg       0.77      0.57      0.54      2115\n",
      "\n",
      "The accuracy for Gaussian Naive Bayes classifier is: 0.4052009456264775\n",
      "Prediction Positive Number: 138 True Number: 1314\n",
      "Prediction Negative Number: 1977 True Number: 801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.95      0.55       801\n",
      "           1       0.70      0.07      0.13      1314\n",
      "\n",
      "    accuracy                           0.41      2115\n",
      "   macro avg       0.54      0.51      0.34      2115\n",
      "weighted avg       0.58      0.41      0.29      2115\n",
      "\n",
      "The accuracy for neural network classifier is: 0.5446808510638298\n",
      "Prediction Positive Number: 389 True Number: 1314\n",
      "Prediction Negative Number: 1726 True Number: 801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.98      0.62       801\n",
      "           1       0.95      0.28      0.43      1314\n",
      "\n",
      "    accuracy                           0.54      2115\n",
      "   macro avg       0.70      0.63      0.53      2115\n",
      "weighted avg       0.76      0.54      0.50      2115\n",
      "\n",
      "The accuracy for LGBM classifier is: 0.6775413711583924\n",
      "Prediction Positive Number: 632 True Number: 1314\n",
      "Prediction Negative Number: 1483 True Number: 801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70       801\n",
      "           1       1.00      0.48      0.65      1314\n",
      "\n",
      "    accuracy                           0.68      2115\n",
      "   macro avg       0.77      0.74      0.68      2115\n",
      "weighted avg       0.83      0.68      0.67      2115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, n_clu=NUM_CLUSTERS, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    count_values(cur_train_y)\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=500)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    \n",
    "    count_values(cur_train_y)\n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_nb, nb_acc = train_nb(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_mlp, mlp_acc = train_mlp(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_lgbm, lgbm_acc = train_lgbm(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from mlp import MLP\n",
    "from util import load_pickle_file\n",
    "from util import upsample_pos\n",
    "num_epochs = 80\n",
    "bs = 100\n",
    "learning_rate = 1e-4\n",
    "net = MLP()\n",
    "def train_nn(x, y, x_test, y_test):\n",
    "    num_bs = len(x) // bs\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(np.array([1., 1.])).float())  \n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    for epoch in range(num_epochs):\n",
    "        for ii in range(num_bs - 1):  \n",
    "            # Convert torch tensor to Variable\n",
    "            curr_data = x[ii * bs: (ii + 1) * bs]\n",
    "            curr_labels = y[ii * bs: (ii + 1) * bs]\n",
    "            # print(curr_data.shape)\n",
    "            # print(type(curr_data.view(-1, 651)))\n",
    "            curr_data = Variable(torch.tensor(curr_data).float())\n",
    "            curr_labels = Variable(torch.tensor(curr_labels).long())\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = net(curr_data)\n",
    "            loss = criterion(outputs, curr_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (ii+1) % 100 == 0:\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, ii+1, len(x)//bs, loss.data))\n",
    "                print(\"train acc:\" + str(1 - len(torch.nonzero(predicted - curr_labels)) * 1.0 / len(predicted)))\n",
    "                test_data = Variable(torch.tensor(x_test).float())\n",
    "                test_labels = Variable(torch.tensor(y_test).long())\n",
    "                outputs = net(test_data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                print(\"test acc:\" + str(1 - len(torch.nonzero(predicted - test_labels)) * 1.0 / len(test_labels)))\n",
    "    test_data = Variable(torch.tensor(x_test).float())\n",
    "    test_labels = Variable(torch.tensor(y_test).long())\n",
    "    outputs = net(test_data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(\"f1: \" + str(f1_score(test_labels, predicted > 0.5, average=None)))\n",
    "    print(\"precision: \" + str(precision_score(test_labels, predicted > 0.5, average=None)))\n",
    "    print(\"recall: \" + str(recall_score(test_labels, predicted > 0.5, average=None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825\n",
      "282686\n",
      "0.3323262839879154\n",
      "[[   0 7946]\n",
      " [   1 6854]]\n",
      "1\n",
      "1\n",
      "[[   0 7946]\n",
      " [   1 6854]]\n",
      "length of train set 14800, test set 3754\n",
      "Epoch [1/80], Step [100/148], Loss: 0.7410\n",
      "train acc:0.41000000000000003\n",
      "test acc:0.4608417687799681\n",
      "Epoch [2/80], Step [100/148], Loss: 0.7015\n",
      "train acc:0.41000000000000003\n",
      "test acc:0.4608417687799681\n",
      "Epoch [3/80], Step [100/148], Loss: 0.6896\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.539158231220032\n",
      "Epoch [4/80], Step [100/148], Loss: 0.6855\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.539158231220032\n",
      "Epoch [5/80], Step [100/148], Loss: 0.6838\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.539158231220032\n",
      "Epoch [6/80], Step [100/148], Loss: 0.6830\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.539158231220032\n",
      "Epoch [7/80], Step [100/148], Loss: 0.6824\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.539158231220032\n",
      "Epoch [8/80], Step [100/148], Loss: 0.6806\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.539158231220032\n",
      "Epoch [9/80], Step [100/148], Loss: 0.6761\n",
      "train acc:0.5900000000000001\n",
      "test acc:0.5394246137453382\n",
      "Epoch [10/80], Step [100/148], Loss: 0.6692\n",
      "train acc:0.7\n",
      "test acc:0.5990942994139584\n",
      "Epoch [11/80], Step [100/148], Loss: 0.6576\n",
      "train acc:0.74\n",
      "test acc:0.6622269579115609\n",
      "Epoch [12/80], Step [100/148], Loss: 0.6421\n",
      "train acc:0.76\n",
      "test acc:0.6699520511454449\n",
      "Epoch [13/80], Step [100/148], Loss: 0.6261\n",
      "train acc:0.76\n",
      "test acc:0.6704848161960575\n",
      "Epoch [14/80], Step [100/148], Loss: 0.6122\n",
      "train acc:0.76\n",
      "test acc:0.6702184336707512\n",
      "Epoch [15/80], Step [100/148], Loss: 0.6016\n",
      "train acc:0.76\n",
      "test acc:0.6704848161960575\n",
      "Epoch [16/80], Step [100/148], Loss: 0.5937\n",
      "train acc:0.76\n",
      "test acc:0.6699520511454449\n",
      "Epoch [17/80], Step [100/148], Loss: 0.5877\n",
      "train acc:0.76\n",
      "test acc:0.6710175812466702\n",
      "Epoch [18/80], Step [100/148], Loss: 0.5831\n",
      "train acc:0.76\n",
      "test acc:0.6718167288225892\n",
      "Epoch [19/80], Step [100/148], Loss: 0.5794\n",
      "train acc:0.77\n",
      "test acc:0.6715503462972829\n",
      "Epoch [20/80], Step [100/148], Loss: 0.5765\n",
      "train acc:0.77\n",
      "test acc:0.6704848161960575\n",
      "Epoch [21/80], Step [100/148], Loss: 0.5741\n",
      "train acc:0.77\n",
      "test acc:0.6715503462972829\n",
      "Epoch [22/80], Step [100/148], Loss: 0.5721\n",
      "train acc:0.77\n",
      "test acc:0.6715503462972829\n",
      "Epoch [23/80], Step [100/148], Loss: 0.5704\n",
      "train acc:0.77\n",
      "test acc:0.6712839637719765\n",
      "Epoch [24/80], Step [100/148], Loss: 0.5690\n",
      "train acc:0.77\n",
      "test acc:0.6712839637719765\n",
      "Epoch [25/80], Step [100/148], Loss: 0.5678\n",
      "train acc:0.77\n",
      "test acc:0.6715503462972829\n",
      "Epoch [26/80], Step [100/148], Loss: 0.5668\n",
      "train acc:0.77\n",
      "test acc:0.6718167288225892\n",
      "Epoch [27/80], Step [100/148], Loss: 0.5659\n",
      "train acc:0.77\n",
      "test acc:0.6720831113478956\n",
      "Epoch [28/80], Step [100/148], Loss: 0.5651\n",
      "train acc:0.77\n",
      "test acc:0.672349493873202\n",
      "Epoch [29/80], Step [100/148], Loss: 0.5644\n",
      "train acc:0.77\n",
      "test acc:0.6712839637719765\n",
      "Epoch [30/80], Step [100/148], Loss: 0.5637\n",
      "train acc:0.77\n",
      "test acc:0.6718167288225892\n",
      "Epoch [31/80], Step [100/148], Loss: 0.5632\n",
      "train acc:0.77\n",
      "test acc:0.672349493873202\n",
      "Epoch [32/80], Step [100/148], Loss: 0.5627\n",
      "train acc:0.77\n",
      "test acc:0.6728822589238146\n",
      "Epoch [33/80], Step [100/148], Loss: 0.5622\n",
      "train acc:0.77\n",
      "test acc:0.6728822589238146\n",
      "Epoch [34/80], Step [100/148], Loss: 0.5618\n",
      "train acc:0.77\n",
      "test acc:0.6734150239744272\n",
      "Epoch [35/80], Step [100/148], Loss: 0.5615\n",
      "train acc:0.77\n",
      "test acc:0.6739477890250399\n",
      "Epoch [36/80], Step [100/148], Loss: 0.5611\n",
      "train acc:0.77\n",
      "test acc:0.6739477890250399\n",
      "Epoch [37/80], Step [100/148], Loss: 0.5608\n",
      "train acc:0.77\n",
      "test acc:0.6736814064997336\n",
      "Epoch [38/80], Step [100/148], Loss: 0.5605\n",
      "train acc:0.77\n",
      "test acc:0.6736814064997336\n",
      "Epoch [39/80], Step [100/148], Loss: 0.5603\n",
      "train acc:0.77\n",
      "test acc:0.6742141715503462\n",
      "Epoch [40/80], Step [100/148], Loss: 0.5600\n",
      "train acc:0.77\n",
      "test acc:0.6750133191262653\n",
      "Epoch [41/80], Step [100/148], Loss: 0.5598\n",
      "train acc:0.77\n",
      "test acc:0.6758124667021843\n",
      "Epoch [42/80], Step [100/148], Loss: 0.5596\n",
      "train acc:0.77\n",
      "test acc:0.675546084176878\n",
      "Epoch [43/80], Step [100/148], Loss: 0.5594\n",
      "train acc:0.77\n",
      "test acc:0.6750133191262653\n",
      "Epoch [44/80], Step [100/148], Loss: 0.5592\n",
      "train acc:0.77\n",
      "test acc:0.6752797016515717\n",
      "Epoch [45/80], Step [100/148], Loss: 0.5590\n",
      "train acc:0.77\n",
      "test acc:0.6750133191262653\n",
      "Epoch [46/80], Step [100/148], Loss: 0.5588\n",
      "train acc:0.77\n",
      "test acc:0.675546084176878\n",
      "Epoch [47/80], Step [100/148], Loss: 0.5587\n",
      "train acc:0.77\n",
      "test acc:0.6752797016515717\n",
      "Epoch [48/80], Step [100/148], Loss: 0.5585\n",
      "train acc:0.78\n",
      "test acc:0.6742141715503462\n",
      "Epoch [49/80], Step [100/148], Loss: 0.5583\n",
      "train acc:0.78\n",
      "test acc:0.674746936600959\n",
      "Epoch [50/80], Step [100/148], Loss: 0.5582\n",
      "train acc:0.78\n",
      "test acc:0.6760788492274907\n",
      "Epoch [51/80], Step [100/148], Loss: 0.5580\n",
      "train acc:0.78\n",
      "test acc:0.6750133191262653\n",
      "Epoch [52/80], Step [100/148], Loss: 0.5579\n",
      "train acc:0.78\n",
      "test acc:0.674746936600959\n",
      "Epoch [53/80], Step [100/148], Loss: 0.5577\n",
      "train acc:0.77\n",
      "test acc:0.6750133191262653\n",
      "Epoch [54/80], Step [100/148], Loss: 0.5576\n",
      "train acc:0.76\n",
      "test acc:0.6742141715503462\n",
      "Epoch [55/80], Step [100/148], Loss: 0.5574\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [56/80], Step [100/148], Loss: 0.5573\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [57/80], Step [100/148], Loss: 0.5571\n",
      "train acc:0.76\n",
      "test acc:0.6734150239744272\n",
      "Epoch [58/80], Step [100/148], Loss: 0.5570\n",
      "train acc:0.76\n",
      "test acc:0.6744805540756527\n",
      "Epoch [59/80], Step [100/148], Loss: 0.5568\n",
      "train acc:0.76\n",
      "test acc:0.6744805540756527\n",
      "Epoch [60/80], Step [100/148], Loss: 0.5567\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [61/80], Step [100/148], Loss: 0.5565\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [62/80], Step [100/148], Loss: 0.5564\n",
      "train acc:0.76\n",
      "test acc:0.6742141715503462\n",
      "Epoch [63/80], Step [100/148], Loss: 0.5562\n",
      "train acc:0.76\n",
      "test acc:0.6742141715503462\n",
      "Epoch [64/80], Step [100/148], Loss: 0.5560\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [65/80], Step [100/148], Loss: 0.5559\n",
      "train acc:0.76\n",
      "test acc:0.6742141715503462\n",
      "Epoch [66/80], Step [100/148], Loss: 0.5557\n",
      "train acc:0.76\n",
      "test acc:0.6744805540756527\n",
      "Epoch [67/80], Step [100/148], Loss: 0.5555\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [68/80], Step [100/148], Loss: 0.5554\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [69/80], Step [100/148], Loss: 0.5552\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [70/80], Step [100/148], Loss: 0.5550\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [71/80], Step [100/148], Loss: 0.5549\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [72/80], Step [100/148], Loss: 0.5547\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [73/80], Step [100/148], Loss: 0.5545\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [74/80], Step [100/148], Loss: 0.5543\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [75/80], Step [100/148], Loss: 0.5542\n",
      "train acc:0.76\n",
      "test acc:0.6739477890250399\n",
      "Epoch [76/80], Step [100/148], Loss: 0.5540\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [77/80], Step [100/148], Loss: 0.5538\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [78/80], Step [100/148], Loss: 0.5537\n",
      "train acc:0.76\n",
      "test acc:0.6736814064997336\n",
      "Epoch [79/80], Step [100/148], Loss: 0.5535\n",
      "train acc:0.76\n",
      "test acc:0.6742141715503462\n",
      "Epoch [80/80], Step [100/148], Loss: 0.5533\n",
      "train acc:0.76\n",
      "test acc:0.6742141715503462\n",
      "f1: [0.69141414 0.65558061]\n",
      "precision: [0.7071281  0.63971397]\n",
      "recall: [0.6763834  0.67225434]\n",
      "[[   0 3313]\n",
      " [   1 5100]]\n",
      "1\n",
      "1\n",
      "[[   0 3313]\n",
      " [   1 5100]]\n",
      "length of train set 8413, test set 2098\n",
      "f1: [0.57029703 0.75718016]\n",
      "precision: [0.62337662 0.72241993]\n",
      "recall: [0.52554745 0.79545455]\n",
      "[[   0 3700]\n",
      " [   1 2417]]\n",
      "1\n",
      "1\n",
      "[[   0 3700]\n",
      " [   1 2417]]\n",
      "length of train set 6117, test set 1483\n",
      "f1: [0.76541962 0.53036437]\n",
      "precision: [0.71146617 0.62529833]\n",
      "recall: [0.82822757 0.46045694]\n",
      "[[   0 4900]\n",
      " [   1 5488]]\n",
      "1\n",
      "1\n",
      "[[   0 4900]\n",
      " [   1 5488]]\n",
      "length of train set 10388, test set 2595\n",
      "Epoch [1/80], Step [100/103], Loss: 0.5950\n",
      "train acc:0.73\n",
      "test acc:0.6736030828516377\n",
      "Epoch [2/80], Step [100/103], Loss: 0.5944\n",
      "train acc:0.72\n",
      "test acc:0.6724470134874759\n",
      "Epoch [3/80], Step [100/103], Loss: 0.5940\n",
      "train acc:0.7\n",
      "test acc:0.6724470134874759\n",
      "Epoch [4/80], Step [100/103], Loss: 0.5936\n",
      "train acc:0.7\n",
      "test acc:0.6728323699421965\n",
      "Epoch [5/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6732177263969171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.7\n",
      "test acc:0.6728323699421965\n",
      "Epoch [7/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.7\n",
      "test acc:0.6751445086705202\n",
      "Epoch [8/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.7\n",
      "test acc:0.6759152215799615\n",
      "Epoch [9/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.69\n",
      "test acc:0.6770712909441233\n",
      "Epoch [10/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.69\n",
      "test acc:0.6751445086705202\n",
      "Epoch [11/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.7\n",
      "test acc:0.6743737957610789\n",
      "Epoch [12/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.7\n",
      "test acc:0.6743737957610789\n",
      "Epoch [13/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.7\n",
      "test acc:0.6751445086705202\n",
      "Epoch [14/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.7\n",
      "test acc:0.6751445086705202\n",
      "Epoch [15/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.7\n",
      "test acc:0.6755298651252408\n",
      "Epoch [16/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.7\n",
      "test acc:0.6774566473988439\n",
      "Epoch [17/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.7\n",
      "test acc:0.6782273603082851\n",
      "Epoch [18/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.7\n",
      "test acc:0.6782273603082851\n",
      "Epoch [19/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.7\n",
      "test acc:0.6770712909441233\n",
      "Epoch [20/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.7\n",
      "test acc:0.6774566473988439\n",
      "Epoch [21/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.7\n",
      "test acc:0.6759152215799615\n",
      "Epoch [22/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.7\n",
      "test acc:0.6759152215799615\n",
      "Epoch [23/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.7\n",
      "test acc:0.6766859344894027\n",
      "Epoch [24/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [25/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [26/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [27/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [28/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [29/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [30/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [31/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [32/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6747591522157996\n",
      "Epoch [33/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [34/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [35/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [36/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [37/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [38/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [39/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [40/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6766859344894027\n",
      "Epoch [41/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6770712909441233\n",
      "Epoch [42/80], Step [100/103], Loss: 0.5935\n",
      "train acc:0.69\n",
      "test acc:0.6778420038535645\n",
      "Epoch [43/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6778420038535645\n",
      "Epoch [44/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6774566473988439\n",
      "Epoch [45/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6770712909441233\n",
      "Epoch [46/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6774566473988439\n",
      "Epoch [47/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6774566473988439\n",
      "Epoch [48/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6770712909441233\n",
      "Epoch [49/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6766859344894027\n",
      "Epoch [50/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.7\n",
      "test acc:0.6774566473988439\n",
      "Epoch [51/80], Step [100/103], Loss: 0.5934\n",
      "train acc:0.69\n",
      "test acc:0.6774566473988439\n",
      "Epoch [52/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.6770712909441233\n",
      "Epoch [53/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.6770712909441233\n",
      "Epoch [54/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [55/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.6799999999999999\n",
      "test acc:0.676300578034682\n",
      "Epoch [56/80], Step [100/103], Loss: 0.5933\n",
      "train acc:0.6799999999999999\n",
      "test acc:0.6755298651252408\n",
      "Epoch [57/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.6799999999999999\n",
      "test acc:0.6759152215799615\n",
      "Epoch [58/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.6799999999999999\n",
      "test acc:0.676300578034682\n",
      "Epoch [59/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.6799999999999999\n",
      "test acc:0.6770712909441233\n",
      "Epoch [60/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [61/80], Step [100/103], Loss: 0.5932\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [62/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [63/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [64/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "Epoch [65/80], Step [100/103], Loss: 0.5931\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [66/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [67/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [68/80], Step [100/103], Loss: 0.5930\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [69/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [70/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.69\n",
      "test acc:0.6751445086705202\n",
      "Epoch [71/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.69\n",
      "test acc:0.6751445086705202\n",
      "Epoch [72/80], Step [100/103], Loss: 0.5929\n",
      "train acc:0.69\n",
      "test acc:0.6747591522157996\n",
      "Epoch [73/80], Step [100/103], Loss: 0.5928\n",
      "train acc:0.69\n",
      "test acc:0.6751445086705202\n",
      "Epoch [74/80], Step [100/103], Loss: 0.5928\n",
      "train acc:0.69\n",
      "test acc:0.6747591522157996\n",
      "Epoch [75/80], Step [100/103], Loss: 0.5928\n",
      "train acc:0.69\n",
      "test acc:0.6755298651252408\n",
      "Epoch [76/80], Step [100/103], Loss: 0.5927\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [77/80], Step [100/103], Loss: 0.5927\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [78/80], Step [100/103], Loss: 0.5927\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [79/80], Step [100/103], Loss: 0.5927\n",
      "train acc:0.69\n",
      "test acc:0.676300578034682\n",
      "Epoch [80/80], Step [100/103], Loss: 0.5926\n",
      "train acc:0.69\n",
      "test acc:0.6759152215799615\n",
      "f1: [0.64935065 0.70139137]\n",
      "precision: [0.65566836 0.69568294]\n",
      "recall: [0.64315353 0.70719424]\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 4\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, n_clu=NUM_CLUSTERS, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    count_values(cur_train_y)\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=500)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    \n",
    "    count_values(cur_train_y)\n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    train_nn(cur_train_x, cur_train_y, cur_test_x, cur_test_y)\n",
    "#     clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "#     clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "#     clf_nb, nb_acc = train_nb(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "#     clf_mlp, mlp_acc = train_mlp(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "#     clf_lgbm, lgbm_acc = train_lgbm(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
