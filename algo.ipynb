{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from util import load_pickle_file\n",
    "from util import save_pickle_file\n",
    "from util import report_test\n",
    "from util import upsample_pos\n",
    "from util import data_preprocessing\n",
    "from util import rand_train_test\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kmeans(x, y, test=None):\n",
    "    kmeans = KMeans(n_clusters=2, random_state=229).fit(x)\n",
    "    \n",
    "    if test is not None:\n",
    "        x_test, y_test = test\n",
    "        # clf_acc = report_test(kmeans, test, \"kmeans\")\n",
    "        # print(kmeans.cluster_centers_)\n",
    "        y_pred = kmeans.predict(x_test)\n",
    "        print((y_pred == y_test).sum()/len(y_test))\n",
    "        return kmeans.labels_, y_pred\n",
    "    return kmeans, test, 'kmeans'\n",
    "\n",
    "def train_svm(x, y, kernel_type, test=None):\n",
    "    clf_svm = SVC(kernel='linear', probability=True)\n",
    "    if kernel_type == 'poly':\n",
    "        clf_svm = SVC(kernel='poly', degree=8, probability=True)\n",
    "    elif kernel_type == 'rbf':\n",
    "        clf_svm = SVC(kernel='rbf', probability=True)\n",
    "    elif kernel_type == 'sigmoid':\n",
    "        clf_svm = SVC(kernel='sigmoid', probability=True)\n",
    "    clf_svm.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_svm, test, \"svm\")\n",
    "        return clf_svm, test, 'svm'\n",
    "    return clf_svm, test, 'svm'\n",
    "\n",
    "def train_lr(x, y, rand_state=229, solver='liblinear',\n",
    "        max_iter=10000, test=None):\n",
    "    clf_lr = LogisticRegression(\n",
    "        random_state=rand_state, solver=solver, max_iter=max_iter, C=0.0001)\n",
    "    # clf_lr = LogisticRegression(C = 0.0001)\n",
    "    clf_lr.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lr, test, \"logistic regression\")\n",
    "        return clf_lr, clf_acc\n",
    "    return clf_lr\n",
    "\n",
    "def train_rand_forest(x, y, n_est=100, max_depth=3, rand_state=229, test=None):\n",
    "    # clf_rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_depth,\n",
    "    #     random_state=rand_state)\n",
    "    clf_rf = RandomForestClassifier(n_estimators = 100, random_state = 50, n_jobs = -1)\n",
    "    clf_rf.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_rf, test, \"random forest\")\n",
    "        return clf_rf, clf_acc\n",
    "    return clf_rf\n",
    "\n",
    "def train_nb(x, y, test=None):\n",
    "    clf_nb = GaussianNB().fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nb, test, \"Gaussian Naive Bayes\")\n",
    "        return clf_nb, clf_acc\n",
    "    return clf_nb\n",
    "\n",
    "def train_mlp(x, y, solver='lbfgs', alpha=1e-4, hls=(10, 40, 40),\n",
    "        rand_state=229, test=None):\n",
    "    clf_nn = MLPClassifier(\n",
    "        solver=solver, alpha=alpha, hidden_layer_sizes=hls,\n",
    "        random_state=rand_state)\n",
    "    clf_nn.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nn, test, \"neural network\")\n",
    "        return clf_nn, clf_acc\n",
    "    return clf_nn\n",
    "\n",
    "def train_lgbm(x, y, test=None):\n",
    "    clf_lgbm = LGBMClassifier(\n",
    "        nthread=4,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=34,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.041545473,\n",
    "        reg_lambda=0.0735294,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=39.3259775,\n",
    "        silent=-1,\n",
    "        verbose=-1, )\n",
    "    clf_lgbm.fit(x, y, verbose=100)\n",
    "\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lgbm, test, \"LGBM\")\n",
    "        return clf_lgbm, clf_acc\n",
    "    return clf_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has been successfully loaded\n"
     ]
    }
   ],
   "source": [
    "training_data_path = './data_processed/training_data.pkl'\n",
    "label_path = './data_processed/training_lbl.pkl'\n",
    "# training_data_path = './data_processed/training_data_processed.pkl'\n",
    "# label_path = './data_processed/training_lbl_processed.pkl'\n",
    "# training_data_path = './data_processed/training_data.pkl'\n",
    "# label_path = './data_processed/training_lbl.pkl'\n",
    "data = load_pickle_file(training_data_path)\n",
    "label = load_pickle_file(label_path)\n",
    "print('Training data has been successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 275)\n",
      "(307511, 275)\n",
      "(307511,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(label)\n",
    "x = data\n",
    "# entries = list(data.columns)\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "# raise\n",
    "x, y = data_preprocessing(x, y, thres=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is starting ... \n",
      "shape of x: (307511, 275)\n"
     ]
    }
   ],
   "source": [
    "lr_acc_ls = []\n",
    "rf_acc_ls = []\n",
    "nb_acc_ls = []\n",
    "nn_acc_ls = []\n",
    "lgbm_acc_ls = []\n",
    "# kf = KFold(n_splits=1, shuffle=True)\n",
    "print('Training is starting ... ')\n",
    "print('shape of x: {}'.format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, x_test, y_test = upsample_pos(x, y, upsample=True)\n",
    "# x, y, x_test, y_test = rand_train_test(x, y)\n",
    "# save_pickle_file(x, \"training_data_up.pkl\")\n",
    "# save_pickle_file(y, \"training_lbl_up.pkl\")\n",
    "# save_pickle_file(x_test, \"testing_data_up.pkl\")\n",
    "# save_pickle_file(y_test, \"testing_lbl_up.pkl\")\n",
    "# x = load_pickle_file('training_data_up.pkl')\n",
    "# y = load_pickle_file('training_lbl_up.pkl')\n",
    "# x_test = load_pickle_file('testing_data_up.pkl')\n",
    "# y_test = load_pickle_file('testing_lbl_up.pkl')\n",
    "# raise\n",
    "# print('Percentage of zeros in trainset input: {}'.format(np.count_nonzero(x==0)/x.size))\n",
    "# print('Number of positive examples: {}, negative: {}'.format((y==1).sum(), (y==0).sum()))\n",
    "# # for train, test in kf.split(x):\n",
    "# print(\"here\")\n",
    "# x_train, x_test, y_train, y_test = x, x_test, y, y_test\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(len(y_test==1))\n",
    "# print(len(y_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x, y, upsample=False, k_neighbors=1000):\n",
    "    # less positive, more negative\n",
    "    all_pos = np.where(y == 1)\n",
    "    print(len(all_pos))\n",
    "    x_all_pos = x[all_pos[0]]\n",
    "    y_all_pos = y[all_pos[0]]\n",
    "\n",
    "    all_neg = np.where(y == 0)\n",
    "    print(len(all_neg))\n",
    "    x_all_neg = x[all_neg[0]]\n",
    "    y_all_neg = y[all_neg[0]]\n",
    "\n",
    "    if upsample:\n",
    "        rand_ind = np.arange(len(x_all_neg))\n",
    "        np.random.shuffle(rand_ind)\n",
    "        x_neg_new = x_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        y_neg_new = y_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        x_all_new = np.concatenate((x_neg_new, x_all_pos), axis=0)\n",
    "        y_all_new = np.concatenate((y_neg_new, y_all_pos), axis=0)\n",
    "        sm = SMOTE(random_state=233333, sampling_strategy=1.0, k_neighbors=k_neighbors)\n",
    "        x_train, y_train = sm.fit_sample(x_all_new, y_all_new)\n",
    "    else:\n",
    "        # undersample: balance train set\n",
    "        x_all_neg = x_all_neg[:int(5*len(x_all_pos))]\n",
    "        y_all_neg = y_all_neg[:int(5*len(x_all_pos))]\n",
    "        x_train = np.concatenate((x_all_neg, x_all_pos), axis=0)\n",
    "        y_train = np.concatenate((y_all_neg, y_all_pos), axis=0)\n",
    "    \n",
    "    rand_shuffle = np.arange(len(x_train))\n",
    "    np.random.shuffle(rand_shuffle)\n",
    "    x_train = x_train[rand_shuffle]\n",
    "    y_train = y_train[rand_shuffle]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19859\n",
      "99295\n",
      "0.4654998740871317\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "length of train set 78797, test set 6702\n",
      "train 1 78797, train 0 78797\n",
      "The accuracy for logistic regression classifier is: 0.4795583407937929\n",
      "Prediction Positive Number: 0 True Number: 3488\n",
      "Prediction Negative Number: 6702 True Number: 3214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65      3214\n",
      "           1       0.00      0.00      0.00      3488\n",
      "\n",
      "    accuracy                           0.48      6702\n",
      "   macro avg       0.24      0.50      0.32      6702\n",
      "weighted avg       0.23      0.48      0.31      6702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for random forest classifier is: 0.8888391524917935\n",
      "Prediction Positive Number: 2745 True Number: 3488\n",
      "Prediction Negative Number: 3957 True Number: 3214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90      3214\n",
      "           1       1.00      0.79      0.88      3488\n",
      "\n",
      "    accuracy                           0.89      6702\n",
      "   macro avg       0.91      0.89      0.89      6702\n",
      "weighted avg       0.91      0.89      0.89      6702\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "length of train set 11316, test set 1240\n",
      "train 1 11316, train 0 11316\n",
      "The accuracy for logistic regression classifier is: 0.6104838709677419\n",
      "Prediction Positive Number: 0 True Number: 483\n",
      "Prediction Negative Number: 1240 True Number: 757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76       757\n",
      "           1       0.00      0.00      0.00       483\n",
      "\n",
      "    accuracy                           0.61      1240\n",
      "   macro avg       0.31      0.50      0.38      1240\n",
      "weighted avg       0.37      0.61      0.46      1240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for random forest classifier is: 0.9112903225806451\n",
      "Prediction Positive Number: 383 True Number: 483\n",
      "Prediction Negative Number: 857 True Number: 757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       757\n",
      "           1       0.99      0.78      0.87       483\n",
      "\n",
      "    accuracy                           0.91      1240\n",
      "   macro avg       0.93      0.89      0.90      1240\n",
      "weighted avg       0.92      0.91      0.91      1240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=1000)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    cur_test_x, cur_test_y = balance_data(cur_test_x, cur_test_y, upsample=False, k_neighbors=1000)\n",
    "    \n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    print('train 1 {}, train 0 {}'.format(len(cur_train_y==1), len(cur_train_y==0)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19859\n",
      "99295\n",
      "0.4654998740871317\n",
      "1\n",
      "1\n",
      "[[    0 28002]\n",
      " [    1 28002]]\n",
      "length of train set 56004, test set 6702\n",
      "train 1 56004, train 0 56004\n",
      "The accuracy for logistic regression classifier is: 0.7555953446732319\n",
      "Prediction Positive Number: 3544 True Number: 3488\n",
      "Prediction Negative Number: 3158 True Number: 3214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74      3214\n",
      "           1       0.76      0.77      0.77      3488\n",
      "\n",
      "    accuracy                           0.76      6702\n",
      "   macro avg       0.76      0.75      0.76      6702\n",
      "weighted avg       0.76      0.76      0.76      6702\n",
      "\n",
      "The accuracy for random forest classifier is: 0.8976424947776783\n",
      "Prediction Positive Number: 2918 True Number: 3488\n",
      "Prediction Negative Number: 3784 True Number: 3214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90      3214\n",
      "           1       0.98      0.82      0.89      3488\n",
      "\n",
      "    accuracy                           0.90      6702\n",
      "   macro avg       0.91      0.90      0.90      6702\n",
      "weighted avg       0.91      0.90      0.90      6702\n",
      "\n",
      "1\n",
      "1\n",
      "[[   0 3772]\n",
      " [   1 3772]]\n",
      "length of train set 7544, test set 1240\n",
      "train 1 7544, train 0 7544\n",
      "The accuracy for logistic regression classifier is: 0.7104838709677419\n",
      "Prediction Positive Number: 554 True Number: 483\n",
      "Prediction Negative Number: 686 True Number: 757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.75       757\n",
      "           1       0.61      0.70      0.65       483\n",
      "\n",
      "    accuracy                           0.71      1240\n",
      "   macro avg       0.70      0.71      0.70      1240\n",
      "weighted avg       0.72      0.71      0.71      1240\n",
      "\n",
      "The accuracy for random forest classifier is: 0.9040322580645161\n",
      "Prediction Positive Number: 426 True Number: 483\n",
      "Prediction Negative Number: 814 True Number: 757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       757\n",
      "           1       0.93      0.82      0.87       483\n",
      "\n",
      "    accuracy                           0.90      1240\n",
      "   macro avg       0.91      0.89      0.90      1240\n",
      "weighted avg       0.91      0.90      0.90      1240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, test=[x_test, y_test])\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=True, k_neighbors=500)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    \n",
    "    b = np.bincount(cur_train_y)\n",
    "    ii = np.nonzero(b)[0]\n",
    "    print(np.vstack((ii,b[ii])).T)\n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
