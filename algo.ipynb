{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from util import load_pickle_file\n",
    "from util import save_pickle_file\n",
    "from util import report_test\n",
    "from util import upsample_pos\n",
    "from util import data_preprocessing\n",
    "from util import rand_train_test\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(clf, test, clf_name):\n",
    "    x_test, y_test = test\n",
    "    \n",
    "    fpr = list()\n",
    "    tpr = list()\n",
    "    aucs = list()\n",
    "    for i in range(len(clf)):\n",
    "        fpr, tpr, _ = roc_curve(y_test, clf[i].predict_proba(x_test)[:,1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, label='ROC curve for ' + clf_name[i] + ' (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.legend(loc=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gboost(x, y, test=None):\n",
    "    clf = ensemble.GradientBoostingClassifier(n_estimators=1000, max_leaf_nodes=4, max_depth=None, random_state=2, min_samples_split=5)\n",
    "    clf.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf, test, \"gradient boosting\")\n",
    "        return clf, clf_acc\n",
    "    return clf, test, \"gradient boosting\"\n",
    "\n",
    "def train_kmeans(x, y, test=None, k=2):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=229).fit(x)\n",
    "    \n",
    "    if test is not None:\n",
    "        x_test, y_test = test\n",
    "        # clf_acc = report_test(kmeans, test, \"kmeans\")\n",
    "        # print(kmeans.cluster_centers_)\n",
    "        y_pred = kmeans.predict(x_test)\n",
    "        print((y_pred == y_test).sum()/len(y_test))\n",
    "        return kmeans.labels_, y_pred\n",
    "    return kmeans, test, 'kmeans'\n",
    "\n",
    "def train_svm(x, y, kernel_type, test=None):\n",
    "    clf_svm = SVC(kernel='linear', probability=True)\n",
    "    if kernel_type == 'poly':\n",
    "        clf_svm = SVC(kernel='poly', degree=8, probability=True)\n",
    "    elif kernel_type == 'rbf':\n",
    "        clf_svm = SVC(kernel='rbf', probability=True)\n",
    "    elif kernel_type == 'sigmoid':\n",
    "        clf_svm = SVC(kernel='sigmoid', probability=True)\n",
    "    clf_svm.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_svm, test, \"svm\")\n",
    "        return clf_svm, test, 'svm'\n",
    "    return clf_svm, test, 'svm'\n",
    "\n",
    "def train_lr(x, y, rand_state=44, solver='liblinear',\n",
    "        max_iter=10000, test=None):\n",
    "    clf_lr = LogisticRegression(\n",
    "        random_state=rand_state, solver=solver, max_iter=max_iter, C=0.0001)\n",
    "    # clf_lr = LogisticRegression(C = 0.0001)\n",
    "    clf_lr.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lr, test, \"logistic regression\")\n",
    "        return clf_lr, clf_acc\n",
    "    return clf_lr, test, 'logistic regression'\n",
    "\n",
    "def train_rand_forest(x, y, n_est=100, max_depth=3, rand_state=44, test=None):\n",
    "    # clf_rf = RandomForestClassifier(n_estimators=n_est, max_depth=max_depth,\n",
    "    #     random_state=rand_state)\n",
    "    clf_rf = RandomForestClassifier(n_estimators = 100, random_state = 50, n_jobs = -1)\n",
    "    clf_rf.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_rf, test, \"random forest\")\n",
    "        return clf_rf, clf_acc\n",
    "    return clf_rf, test, \"random forest\"\n",
    "\n",
    "def train_nb(x, y, test=None):\n",
    "    clf_nb = GaussianNB().fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nb, test, \"Gaussian Naive Bayes\")\n",
    "        return clf_nb, clf_acc\n",
    "    return clf_nb, test, \"Gaussian Naive Bayes\"\n",
    "\n",
    "def train_mlp(x, y, solver='lbfgs', alpha=1e-4, hls=(10, 40, 40),\n",
    "        rand_state=229, test=None):\n",
    "    clf_nn = MLPClassifier(\n",
    "        solver=solver, alpha=alpha, hidden_layer_sizes=hls,\n",
    "        random_state=rand_state)\n",
    "    clf_nn.fit(x, y)\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_nn, test, \"neural network\")\n",
    "        return clf_nn, clf_acc\n",
    "    return clf_nn\n",
    "\n",
    "def train_lgbm(x, y, test=None):\n",
    "    clf_lgbm = LGBMClassifier(\n",
    "        nthread=4,\n",
    "        n_estimators=10000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=34,\n",
    "        colsample_bytree=0.9497036,\n",
    "        subsample=0.8715623,\n",
    "        max_depth=8,\n",
    "        reg_alpha=0.041545473,\n",
    "        reg_lambda=0.0735294,\n",
    "        min_split_gain=0.0222415,\n",
    "        min_child_weight=39.3259775,\n",
    "        silent=-1,\n",
    "        verbose=-1, )\n",
    "    clf_lgbm.fit(x, y, verbose=100)\n",
    "\n",
    "    if test is not None:\n",
    "        clf_acc = report_test(clf_lgbm, test, \"LGBM\")\n",
    "        return clf_lgbm, clf_acc\n",
    "    return clf_lgbm, test, \"LGBM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has been successfully loaded\n"
     ]
    }
   ],
   "source": [
    "training_data_path = './data_processed/training_data.pkl'\n",
    "label_path = './data_processed/training_lbl.pkl'\n",
    "# training_data_path = './data_processed/training_data_processed.pkl'\n",
    "# label_path = './data_processed/training_lbl_processed.pkl'\n",
    "# training_data_path = './data_processed/training_data.pkl'\n",
    "# label_path = './data_processed/training_lbl.pkl'\n",
    "data = load_pickle_file(training_data_path)\n",
    "label = load_pickle_file(label_path)\n",
    "print('Training data has been successfully loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 613)\n",
      "(307511, 613)\n",
      "(307511,)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(label)\n",
    "x = data\n",
    "# entries = list(data.columns)\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "# raise\n",
    "x, y = data_preprocessing(x, y, thres=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is starting ... \n",
      "shape of x: (307511, 613)\n"
     ]
    }
   ],
   "source": [
    "lr_acc_ls = []\n",
    "rf_acc_ls = []\n",
    "nb_acc_ls = []\n",
    "nn_acc_ls = []\n",
    "lgbm_acc_ls = []\n",
    "# kf = KFold(n_splits=1, shuffle=True)\n",
    "print('Training is starting ... ')\n",
    "print('shape of x: {}'.format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, x_test, y_test = upsample_pos(x, y, upsample=True)\n",
    "# x, y, x_test, y_test = rand_train_test(x, y)\n",
    "# save_pickle_file(x, \"training_data_up.pkl\")\n",
    "# save_pickle_file(y, \"training_lbl_up.pkl\")\n",
    "# save_pickle_file(x_test, \"testing_data_up.pkl\")\n",
    "# save_pickle_file(y_test, \"testing_lbl_up.pkl\")\n",
    "# x = load_pickle_file('training_data_up.pkl')\n",
    "# y = load_pickle_file('training_lbl_up.pkl')\n",
    "# x_test = load_pickle_file('testing_data_up.pkl')\n",
    "# y_test = load_pickle_file('testing_lbl_up.pkl')\n",
    "# raise\n",
    "# print('Percentage of zeros in trainset input: {}'.format(np.count_nonzero(x==0)/x.size))\n",
    "# print('Number of positive examples: {}, negative: {}'.format((y==1).sum(), (y==0).sum()))\n",
    "# # for train, test in kf.split(x):\n",
    "# print(\"here\")\n",
    "# x_train, x_test, y_train, y_test = x, x_test, y, y_test\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(len(y_test==1))\n",
    "# print(len(y_test==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x, y, upsample=False, k_neighbors=1000):\n",
    "    # less positive, more negative\n",
    "    all_pos = np.where(y == 1)\n",
    "    print(len(all_pos))\n",
    "    x_all_pos = x[all_pos[0]]\n",
    "    y_all_pos = y[all_pos[0]]\n",
    "\n",
    "    all_neg = np.where(y == 0)\n",
    "    print(len(all_neg))\n",
    "    x_all_neg = x[all_neg[0]]\n",
    "    y_all_neg = y[all_neg[0]]\n",
    "\n",
    "    if upsample:\n",
    "        rand_ind = np.arange(len(x_all_neg))\n",
    "        np.random.shuffle(rand_ind)\n",
    "        x_neg_new = x_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        y_neg_new = y_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        x_all_new = np.concatenate((x_neg_new, x_all_pos), axis=0)\n",
    "        y_all_new = np.concatenate((y_neg_new, y_all_pos), axis=0)\n",
    "        sm = SMOTE(random_state=233333, sampling_strategy=1.0, k_neighbors=k_neighbors)\n",
    "        x_train, y_train = sm.fit_sample(x_all_new, y_all_new)\n",
    "    else:\n",
    "        # undersample: balance train set\n",
    "#         x_all_neg = x_all_neg[:int(5*len(x_all_pos))]\n",
    "#         y_all_neg = y_all_neg[:int(5*len(x_all_pos))]\n",
    "        x_all_neg = x_all_neg[:len(x_all_pos)]\n",
    "        y_all_neg = y_all_neg[:len(x_all_pos)]\n",
    "        x_train = np.concatenate((x_all_neg, x_all_pos), axis=0)\n",
    "        y_train = np.concatenate((y_all_neg, y_all_pos), axis=0)\n",
    "    \n",
    "    rand_shuffle = np.arange(len(x_train))\n",
    "    np.random.shuffle(rand_shuffle)\n",
    "    x_train = x_train[rand_shuffle]\n",
    "    y_train = y_train[rand_shuffle]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(x):\n",
    "    b = np.bincount(x)\n",
    "    ii = np.nonzero(b)[0]\n",
    "    print(np.vstack((ii,b[ii])).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24825\n",
      "282686\n",
      "0.4689828801611279\n",
      "[[    0 16207]\n",
      " [    1 17489]]\n",
      "1\n",
      "1\n",
      "[[    0 16207]\n",
      " [    1 17489]]\n",
      "length of train set 33696, test set 8382\n",
      "The accuracy for logistic regression classifier is: 0.6160820806490098\n",
      "Prediction Positive Number: 5975 True Number: 4345\n",
      "Prediction Negative Number: 2407 True Number: 4037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      4037\n",
      "           1       0.52      1.00      0.68      4345\n",
      "\n",
      "    accuracy                           0.52      8382\n",
      "   macro avg       0.76      0.50      0.34      8382\n",
      "weighted avg       0.75      0.52      0.35      8382\n",
      "\n",
      "The accuracy for random forest classifier is: 0.650560725363875\n",
      "Prediction Positive Number: 4498 True Number: 4345\n",
      "Prediction Negative Number: 3884 True Number: 4037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.31      0.43      4037\n",
      "           1       0.58      0.89      0.70      4345\n",
      "\n",
      "    accuracy                           0.61      8382\n",
      "   macro avg       0.65      0.60      0.57      8382\n",
      "weighted avg       0.65      0.61      0.57      8382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "x_train, y_train, x_test, y_test = upsample_pos(x, y, upsample=False)\n",
    "train_group, test_group = train_kmeans(x_train, y_train, test=[x_test, y_test], k=NUM_CLUSTERS)\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_train_x, cur_train_y = x_train[train_group==i], y_train[train_group==i]\n",
    "    count_values(cur_train_y)\n",
    "    cur_train_x, cur_train_y = balance_data(cur_train_x, cur_train_y, upsample=False, k_neighbors=500)\n",
    "    \n",
    "    cur_test_x, cur_test_y = x_test[test_group==i], y_test[test_group==i]\n",
    "    \n",
    "    count_values(cur_train_y)\n",
    "    print('length of train set {}, test set {}'.format(len(cur_train_x), len(cur_test_x)))\n",
    "    clf_lr, lr_acc = train_lr(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf_rf, rf_acc = train_rand_forest(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])\n",
    "    clf = train_gboost(cur_train_x, cur_train_y, test=[cur_test_x, cur_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kmeans_x(x, k=2):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=44).fit(x)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_kmeans_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-66753066c3bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_CLUSTERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_kmeans_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLUSTERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLUSTERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcur_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_kmeans_x' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_CLUSTERS = 2\n",
    "\n",
    "groups = train_kmeans_x(x, k=NUM_CLUSTERS)\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cur_x, cur_y = x[groups==i], y[groups==i]\n",
    "    count_values(cur_y)\n",
    "    x_train, y_train, x_test, y_test = upsample(cur_x, cur_y, upsample=True)\n",
    "    count_values(y_train)\n",
    "    train_lr(x_train, y_train, test=[x_test, y_test])\n",
    "    train_rand_forest(x_train, y_train, test=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(x, y, upsample=True):\n",
    "    # less positive, more negative\n",
    "    all_pos = np.where(y == 1)\n",
    "    print(len(all_pos[0]))\n",
    "    x_all_pos = x[all_pos[0]]\n",
    "    y_all_pos = y[all_pos[0]]\n",
    "    cut_len = len(x_all_pos) // 5\n",
    "    x_test = x_all_pos[:cut_len]\n",
    "    y_test = y_all_pos[:cut_len]\n",
    "    x_all_pos = x_all_pos[cut_len + 1:]\n",
    "    y_all_pos = y_all_pos[cut_len + 1:]\n",
    "\n",
    "    all_neg = np.where(y == 0)\n",
    "    print(len(all_neg[0]))\n",
    "    x_all_neg = x[all_neg[0]]\n",
    "    y_all_neg = y[all_neg[0]]\n",
    "    x_test = np.concatenate((x_test, x_all_neg[:cut_len]), axis=0)\n",
    "    y_test = np.concatenate((y_test, y_all_neg[:cut_len]), axis=0)\n",
    "    x_all_neg = x_all_neg[cut_len + 1:]\n",
    "    y_all_neg = y_all_neg[cut_len + 1:]\n",
    "\n",
    "    if upsample:\n",
    "        rand_ind = np.arange(len(x_all_neg))\n",
    "        np.random.shuffle(rand_ind)\n",
    "        x_neg_new = x_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        y_neg_new = y_all_neg[rand_ind[:2*len(x_all_pos)]]\n",
    "        x_all_new = np.concatenate((x_neg_new, x_all_pos), axis=0)\n",
    "        y_all_new = np.concatenate((y_neg_new, y_all_pos), axis=0)\n",
    "        sm = SMOTE(random_state=233333, sampling_strategy=1.0, k_neighbors=1000)\n",
    "        x_train, y_train = sm.fit_sample(x_all_new, y_all_new)\n",
    "    else:\n",
    "        # undersample: balance train set\n",
    "        x_all_neg = x_all_neg[:int(len(x_all_pos))]\n",
    "        y_all_neg = y_all_neg[:int(len(x_all_pos))]\n",
    "        x_train = np.concatenate((x_all_neg, x_all_pos), axis=0)\n",
    "        y_train = np.concatenate((y_all_neg, y_all_pos), axis=0)\n",
    "    \n",
    "    rand_shuffle = np.arange(len(x_train))\n",
    "    np.random.shuffle(rand_shuffle)\n",
    "    x_train = x_train[rand_shuffle]\n",
    "    y_train = y_train[rand_shuffle]\n",
    "    \n",
    "    rand_shuffle_test = np.arange(len(x_test))\n",
    "    np.random.shuffle(rand_shuffle_test)\n",
    "    x_test = x_test[rand_shuffle_test]\n",
    "    y_test = y_test[rand_shuffle_test]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
